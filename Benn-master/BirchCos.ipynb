{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BirchCos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1w1OP95oLaQiAN2ID83L-hvPYbNRzv8qh",
      "authorship_tag": "ABX9TyNoXRLbzLAZ1buTymjNIOth",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgdshkovi/Benn/blob/master/BirchCos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUh9Zj8KOgaP",
        "colab_type": "code",
        "outputId": "fab4250d-fece-42b8-c514-dafae0bb0255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Z/filter_pruning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Z/filter_pruning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXiovxDcOqo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "import os\n",
        "\n",
        "import math\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from models import *\n",
        "#import spherical_kmeans_old as skm\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import Birch\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from matplotlib import pyplot\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "cos = nn.CosineSimilarity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzC-LtjJOql1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batch_size = 256\n",
        "dataset = 'cifar10'\n",
        "cfg = [32, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 256, 256, 256, 'M', 256, 256, 256]\n",
        "filtered_cfg = [16, 32, 'M', 64, 64, 'M', 128, 128, 128, 'M', 128, 128, 128, 'M', 128, 128, 128]\n",
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSThDpoZOqit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('Main_logs/model_best.pth.tar')\n",
        "checkpoint = torch.load('Main_logs/checkpoint.pth.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSOGFI1VOqg4",
        "colab_type": "code",
        "outputId": "4236f5ee-380e-426c-c5a0-c8a42abb8381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = vgg(dataset='cifar10', depth=16)  #,cfg=cfg)\n",
        "model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPTesXsVOqfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model):\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if True else {}\n",
        "    if dataset == 'cifar10':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    elif dataset == 'cifar100':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"No valid dataset is given.\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    return correct / float(len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rFwdhr1Oqdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def list_duplicates(seq):\n",
        "    tally = defaultdict(list)\n",
        "    for i,item in enumerate(seq):\n",
        "        tally[item].append(i)\n",
        "    return ((key,locs) for key,locs in tally.items() if len(locs)>1)\n",
        "\n",
        "\n",
        "def return_mask(wts, labels, thres):\n",
        "\tmodsource = labels.copy()\n",
        "\t#print(wts)\n",
        "\t#print(labels)\n",
        "\tfor dup in sorted(list_duplicates(labels)):\n",
        "\t    #print(dup)\n",
        "\t    lis = dup[1][1:]\n",
        "\t    a = torch.from_numpy(np.reshape(wts[dup[1][0]],(1,wts[dup[1][0]].size)))\n",
        "\t    #a = wts[dup[1][0]]\n",
        "\t    for i in lis:\n",
        "\t    \tb = torch.from_numpy(np.reshape(wts[i],(1,wts[i].size)))\n",
        "\t    \t#print\n",
        "\t    \tsimi = (cos(a,b))\n",
        "\t    \tif simi>thres:\n",
        "\t    \t  modsource[i] = -1\n",
        "\n",
        "\tmask = []\n",
        "\tfor el in modsource:\n",
        "\t\tif el!=-1:\n",
        "\t\t\tmask.append(1)\n",
        "\t\telse:\n",
        "\t\t\tmask.append(0)\n",
        "\treturn mask\n",
        "\n",
        "def calc_distance(x1, y1, a, b, c):\n",
        "  d = abs((a * x1 + b * y1 + c)) / (math.sqrt(a * a + b * b))\n",
        "  return d\n",
        "\n",
        "ress = []\n",
        "\n",
        "def optimumk(X, shp):\n",
        "  K = list(range(1,shp//2+1))\n",
        "  K.append(shp)\n",
        "  dist_points_from_cluster_center = []\n",
        "  for no_of_clusters in K:\n",
        "    res = skm.spherical_k_means(X,n_clusters=no_of_clusters,random_state=10)\n",
        "    ress.append(res[1])\n",
        "    dist_points_from_cluster_center.append(res[2])\n",
        "\n",
        "  a = dist_points_from_cluster_center[0] - dist_points_from_cluster_center[shp//2]\n",
        "  b = K[shp//2] - K[0]\n",
        "  c1 = K[0] * dist_points_from_cluster_center[shp//2]\n",
        "  c2 = K[shp//2] * dist_points_from_cluster_center[0]\n",
        "  c = c1 - c2\n",
        "\n",
        "  distance_of_points_from_line = []\n",
        "  for k in range(shp//2+1):\n",
        "    distance_of_points_from_line.append(\n",
        "        calc_distance(K[k], dist_points_from_cluster_center[k], a, b, c))\n",
        "  \n",
        "  optk = distance_of_points_from_line.index(max(distance_of_points_from_line))\n",
        "  return optk\n",
        "\n",
        "\n",
        "def return_cluster_labels(X, shp,thres,bf):  #no_of_clus):\n",
        "  brc = Birch(branching_factor=bf, n_clusters=None, threshold=thres)\n",
        "  brc.fit(X)\n",
        "  labels = brc.predict(X)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q_rnBNzGBAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def formclus(X, noc):\n",
        "  mod = KMeans(n_clusters=noc)\n",
        "  mod.fit(X)\n",
        "  yhat = mod.predict(X)\n",
        "  clusters = unique(yhat)\n",
        "  for cluster in clusters:\n",
        "    row_ix = where(yhat == cluster)\n",
        "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5HcsEICG1gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def applyTSNE(X):\n",
        "  shp = X.shape[1]\n",
        "  if shp<50:\n",
        "    noc = shp\n",
        "  else:\n",
        "    noc = 50\n",
        "  pca50 = PCA(n_components=noc)\n",
        "  pca50res = pca50.fit_transform(X)\n",
        "  fashion_pca_tsne = TSNE(random_state=10).fit_transform(pca50res)\n",
        "  xs = fashion_pca_tsne[:,0]\n",
        "  ys = fashion_pca_tsne[:,1]\n",
        "\n",
        "  Y = []\n",
        "  for x,y in zip(xs,ys):\n",
        "\t  Y.append([x,y])\n",
        "  Y = np.array(Y)\n",
        "  return Y\n",
        "\n",
        "  #find opt k here\n",
        "  formclus(Y, noc=k)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd1dEYfoOqZo",
        "colab_type": "code",
        "outputId": "67a9038f-85c1-4b00-a163-dacda941abaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "cos_cfg = []\n",
        "cfg_mask = []\n",
        "layer_id = 0\n",
        "\n",
        "COS_THRES = 0.1\n",
        "BIR_THRES = 0.1\n",
        "BF = 50\n",
        "while COS_THRES<0.7:\n",
        "    BIR_THRES = 0.25\n",
        "    cos_cfg = []\n",
        "    for m in model.modules():\n",
        "      if isinstance(m , nn.Conv2d):\n",
        "        shape = m.weight.data.shape\n",
        "        #print(shape)\n",
        "        reshaped_tensor = m.weight.data.clone().numpy().reshape(shape[0] , shape[1]*shape[2]*shape[3])\n",
        "\n",
        "        ##Y = applyTSNE(reshaped_tensor)\n",
        "\n",
        "        labels = return_cluster_labels(reshaped_tensor,shape[0],thres=BIR_THRES,bf=50)  #filtered_cfg[layer_id])\n",
        "        mask = return_mask(reshaped_tensor, labels, thres=COS_THRES)\n",
        "        #print(mask)\n",
        "        ##k = len(set(labels))\n",
        "        #print(len(set(labels)))\n",
        "        #print(sum(mask))\n",
        "        cos_cfg.append(sum(mask))\n",
        "        cfg_mask.append(torch.tensor(mask))\n",
        "\n",
        "        ##formclus(Y, noc=k)\n",
        "        ##plt.show()\n",
        "            \n",
        "        layer_id += 1\n",
        "        #break\n",
        "      elif isinstance(m, nn.MaxPool2d):\n",
        "        layer_id += 1\n",
        "        cos_cfg.append('M')\n",
        "      #if layer_id>3:\n",
        "        #break\n",
        "    print('COS_THRES, BIR_THRES --- ',end='')\n",
        "    print(COS_THRES, BIR_THRES)\n",
        "    print(cos_cfg)\n",
        "    \n",
        "    COS_THRES += 0.05\n",
        "    print()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COS_THRES, BIR_THRES --- 0.1 0.25\n",
            "[41, 63, 'M', 128, 128, 'M', 256, 256, 256, 'M', 483, 318, 299, 'M', 299, 352, 308]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.15000000000000002 0.25\n",
            "[41, 63, 'M', 128, 128, 'M', 256, 256, 256, 'M', 483, 321, 331, 'M', 347, 386, 345]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.2 0.25\n",
            "[41, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 483, 328, 352, 'M', 385, 412, 364]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.25 0.25\n",
            "[43, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 483, 338, 392, 'M', 420, 432, 381]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.3 0.25\n",
            "[43, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 485, 346, 405, 'M', 443, 455, 395]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.35 0.25\n",
            "[44, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 485, 356, 422, 'M', 471, 471, 406]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.39999999999999997 0.25\n",
            "[47, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 485, 372, 439, 'M', 487, 482, 413]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.44999999999999996 0.25\n",
            "[49, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 489, 387, 447, 'M', 498, 487, 427]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.49999999999999994 0.25\n",
            "[49, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 492, 408, 462, 'M', 502, 495, 435]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.5499999999999999 0.25\n",
            "[51, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 493, 429, 475, 'M', 505, 501, 443]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.6 0.25\n",
            "[53, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 497, 445, 484, 'M', 506, 504, 446]\n",
            "\n",
            "COS_THRES, BIR_THRES --- 0.65 0.25\n",
            "[54, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 504, 464, 496, 'M', 507, 507, 448]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C70imyBZEvfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cos thres = 0.5\n",
        "Main,bf=50\n",
        "[64, 64, 'M', 20, 20, 'M', 45, 55, 38, 'M', 241, 236, 213, 'M', 255, 235, 209]---Y,0.9\n",
        "[64, 64, 'M', 128, 29, 'M', 86, 80, 256, 'M', 339, 345, 332, 'M', 372, 380, 333]---Y,0.5\n",
        "[49, 63, 'M', 127, 128, 'M', 256, 256, 256, 'M', 512, 509, 509, 'M', 502, 495, 435]---0.9\n",
        "[49, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 490, 501, 509, 'M', 502, 495, 435]---0.5\n",
        "[49, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 492, 408, 462, 'M', 502, 495, 435]---0.25\n",
        "[50, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 497, 485, 'M', 316, 264, 219]---0.1\n",
        "\n",
        "L1,bf=50\n",
        "[32, 64, 'M', 17, 17, 'M', 52, 57, 256, 'M', 114, 121, 106, 'M', 104, 95, 81]      ---Y,0.9\n",
        "[32, 64, 'M', 128, 128, 'M', 93, 78, 256, 'M', 179, 178, 163, 'M', 174, 171, 147]  ---Y,0.5\n",
        "[28, 62, 'M', 128, 128, 'M', 256, 256, 256, 'M', 256, 253, 252, 'M', 250, 240, 214]---  1.5\n",
        "[28, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 248, 252, 252, 'M', 250, 240, 214]---  0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDtQEm3LojJj",
        "colab_type": "code",
        "outputId": "b2a3aba5-7cd7-4512-90d5-3e7a9c386ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(cos_cfg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[56, 62, 'M', 128, 128, 'M', 256, 256, 256, 'M', 510, 438, 417, 'M', 173, 160, 169]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD4tz0ECR5aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cos = nn.CosineSimilarity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXNdDVlvRtOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.from_numpy(np.reshape(X[0],(1,X[0].size)))\n",
        "b = torch.from_numpy(np.reshape(X[5],(1,X[5].size)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YLbuGZ3OqWJ",
        "colab_type": "code",
        "outputId": "3ff2de2f-2f3c-4664-ead2-6ebaf411b91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "newmodel = vgg(dataset = 'cifar10' ,cfg=cos_cfg)\n",
        "newmodel.cuda()\n",
        "\n",
        "start_mask = torch.ones(3)\n",
        "layer_id_in_cfg = 0\n",
        "end_mask = cfg_mask[layer_id_in_cfg]\n",
        "for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
        "    if isinstance(m0, nn.BatchNorm2d):\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1,(1,))\n",
        "        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "        m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "        layer_id_in_cfg += 1\n",
        "        start_mask = end_mask\n",
        "        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "            end_mask = cfg_mask[layer_id_in_cfg]\n",
        "    elif isinstance(m0, nn.Conv2d):\n",
        "        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "        if idx0.size == 1:\n",
        "            idx0 = np.resize(idx0, (1,))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1, (1,))\n",
        "        w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
        "        w1 = w1[idx1.tolist(), :, :, :].clone()\n",
        "        m1.weight.data = w1.clone()\n",
        "    elif isinstance(m0, nn.Linear):\n",
        "        if layer_id_in_cfg == len(cfg_mask):\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(cfg_mask[-1].cpu().numpy())))\n",
        "            if idx0.size == 1:\n",
        "                idx0 = np.resize(idx0, (1,))\n",
        "            m1.weight.data = m0.weight.data[:, idx0].clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            continue\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "    elif isinstance(m0, nn.BatchNorm1d):\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "        m1.running_mean = m0.running_mean.clone()\n",
        "        m1.running_var = m0.running_var.clone()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In shape: 3, Out shape 56.\n",
            "In shape: 56, Out shape 62.\n",
            "In shape: 62, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 510.\n",
            "In shape: 510, Out shape 438.\n",
            "In shape: 438, Out shape 417.\n",
            "In shape: 417, Out shape 173.\n",
            "In shape: 173, Out shape 160.\n",
            "In shape: 160, Out shape 169.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTqxhffNOqSi",
        "colab_type": "code",
        "outputId": "f8af0f05-abb3-4793-dd6d-4b6fd1d59fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.save({'cfg': cos_cfg, 'state_dict': newmodel.state_dict()},'Xmeans_logs/0.4/pruned.pth.tar')\n",
        "print(newmodel)\n",
        "model = newmodel\n",
        "model.cuda()\n",
        "acc = test(model)\n",
        "\n",
        "num_parameters = sum([param.nelement() for param in newmodel.parameters()])\n",
        "with open( \"Xmeans_logs/0.4/prune.txt\", \"w\") as fp:\n",
        "    fp.write(\"Number of parameters: \\n\"+str(num_parameters)+\"\\n\")\n",
        "    fp.write(\"Test accuracy: \\n\"+str(acc)+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vgg(\n",
            "  (feature): Sequential(\n",
            "    (0): Conv2d(3, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(62, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (25): BatchNorm2d(510, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(510, 438, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (28): BatchNorm2d(438, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(438, 417, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (31): BatchNorm2d(417, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(417, 173, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (35): BatchNorm2d(173, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(173, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (38): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(160, 169, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (41): BatchNorm2d(169, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=169, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test set: Accuracy: 1629/10000 (16.3%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDCzLOy_OqPK",
        "colab_type": "code",
        "outputId": "e49a7de7-ea4a-4744-ca62-94758df22049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python main_finetune.py --refine Filter_logs/pruned.pth.tar --dataset cifar10 --epochs 10 --save Filter_logs/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 3.237350\n",
            "Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 0.726783\n",
            "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.547180\n",
            "Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 0.646843\n",
            "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.271935\n",
            "Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 0.413913\n",
            "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.418347\n",
            "Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 0.522038\n",
            "main_finetune.py:153: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "\n",
            "Test set: Average loss: 0.4774, Accuracy: 8480/10000 (84.8%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.342160\n",
            "Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 0.275073\n",
            "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.421768\n",
            "Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 0.401331\n",
            "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.335551\n",
            "Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 0.383089\n",
            "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.261973\n",
            "Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 0.254669\n",
            "\n",
            "Test set: Average loss: 0.4138, Accuracy: 8678/10000 (86.8%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.266819\n",
            "Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 0.221399\n",
            "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.337173\n",
            "Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 0.085399\n",
            "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.177809\n",
            "Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 0.333114\n",
            "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.370555\n",
            "Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 0.293188\n",
            "\n",
            "Test set: Average loss: 0.3844, Accuracy: 8810/10000 (88.1%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.096257\n",
            "Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 0.164555\n",
            "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.138722\n",
            "Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 0.204903\n",
            "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.181059\n",
            "Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 0.199728\n",
            "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.149256\n",
            "Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 0.219510\n",
            "\n",
            "Test set: Average loss: 0.3727, Accuracy: 8862/10000 (88.6%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.113797\n",
            "Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 0.160300\n",
            "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.219459\n",
            "Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 0.177086\n",
            "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.220789\n",
            "Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 0.153303\n",
            "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.169538\n",
            "Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 0.130437\n",
            "\n",
            "Test set: Average loss: 0.3636, Accuracy: 8876/10000 (88.8%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.172971\n",
            "Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 0.106796\n",
            "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.168357\n",
            "Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 0.181813\n",
            "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.198001\n",
            "Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.142003\n",
            "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.227023\n",
            "Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.097383\n",
            "\n",
            "Test set: Average loss: 0.3583, Accuracy: 8919/10000 (89.2%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.182942\n",
            "Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.262168\n",
            "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.141101\n",
            "Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.186639\n",
            "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.145180\n",
            "Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 0.164335\n",
            "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.338414\n",
            "Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.255004\n",
            "\n",
            "Test set: Average loss: 0.3615, Accuracy: 8924/10000 (89.2%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.253661\n",
            "Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.217781\n",
            "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.185188\n",
            "Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 0.161850\n",
            "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.115153\n",
            "Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 0.090115\n",
            "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.211271\n",
            "Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.193146\n",
            "\n",
            "Test set: Average loss: 0.3475, Accuracy: 8942/10000 (89.4%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.177031\n",
            "Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.068579\n",
            "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.076584\n",
            "Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.160076\n",
            "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 0.173006\n",
            "Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 0.247607\n",
            "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.092380\n",
            "Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 0.156644\n",
            "\n",
            "Test set: Average loss: 0.3380, Accuracy: 8986/10000 (89.9%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.212222\n",
            "Train Epoch: 9 [6400/50000 (12.8%)]\tLoss: 0.157991\n",
            "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 0.053095\n",
            "Train Epoch: 9 [19200/50000 (38.4%)]\tLoss: 0.164645\n",
            "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 0.145943\n",
            "Train Epoch: 9 [32000/50000 (63.9%)]\tLoss: 0.203535\n",
            "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 0.137545\n",
            "Train Epoch: 9 [44800/50000 (89.5%)]\tLoss: 0.217458\n",
            "\n",
            "Test set: Average loss: 0.3412, Accuracy: 8994/10000 (89.9%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7B_C0maKB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}