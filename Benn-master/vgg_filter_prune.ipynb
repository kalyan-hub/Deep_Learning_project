{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg-filter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ez-_50ZNvd8-7qgwN95cH2JI9VDI4fD4",
      "authorship_tag": "ABX9TyOZjylC9BiRKy0fhFQJaIph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgdshkovi/Benn/blob/master/vgg_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6prBvjT4rvTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8b1f29a-2e15-48f3-cbeb-88b952bff094"
      },
      "source": [
        "%cd /content/drive/My Drive/Z/filter_pruning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Z/filter_pruning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq-WG0qdrxt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import math\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from models import *\n",
        "import spherical_kmeans as skm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsRGikTVrxpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batch_size = 256\n",
        "dataset = 'cifar10'\n",
        "cfg = [32, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 256, 256, 256, 'M', 256, 256, 256]\n",
        "filtered_cfg = [16, 32, 'M', 64, 64, 'M', 128, 128, 128, 'M', 128, 128, 128, 'M', 128, 128, 128]\n",
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCUjDpUTuLyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('L1_logs/model_best.pth.tar')\n",
        "checkpoint = torch.load('L1_logs/checkpoint.pth.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMVKchfQrxlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbd1ca42-c79c-4992-9b3e-0fc5b07711b3"
      },
      "source": [
        "model = vgg(dataset='cifar10', depth=16,cfg=cfg)\n",
        "model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uYDsU-PWkYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model):\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if True else {}\n",
        "    if dataset == 'cifar10':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    elif dataset == 'cifar100':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"No valid dataset is given.\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    return correct / float(len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7NFzQKsucbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "def list_duplicates(seq):\n",
        "    tally = defaultdict(list)\n",
        "    for i,item in enumerate(seq):\n",
        "        tally[item].append(i)\n",
        "    return ((key,locs) for key,locs in tally.items() if len(locs)>1)\n",
        "\n",
        "\n",
        "def return_mask(labels):\n",
        "\tmodsource = labels.copy()\n",
        "\tfor dup in sorted(list_duplicates(labels)):\n",
        "\t    lis = dup[1][1:]\n",
        "\t    for i in lis:\n",
        "\t    \tmodsource[i] = -1\n",
        "\n",
        "\tmask = []\n",
        "\tfor el in modsource:\n",
        "\t\tif el!=-1:\n",
        "\t\t\tmask.append(1)\n",
        "\t\telse:\n",
        "\t\t\tmask.append(0)\n",
        "\treturn mask\n",
        "\n",
        "\n",
        "def return_cluster_labels(feat_wts,no_of_clus):\n",
        "  res = skm.spherical_k_means(feat_wts,n_clusters=no_of_clus,random_state=10)\n",
        "  return res[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWQWAbz0ucWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "8c29727c-54b7-45e6-8a51-f8482c792391"
      },
      "source": [
        "cfg_mask = []\n",
        "layer_id = 0\n",
        "for m in model.modules():\n",
        "  if isinstance(m , nn.Conv2d):\n",
        "    shape = m.weight.data.shape\n",
        "    print(shape)\n",
        "    reshaped_tensor = m.weight.data.clone().numpy().reshape(shape[0] , shape[1]*shape[2]*shape[3])\n",
        "\n",
        "    labels = return_cluster_labels(reshaped_tensor,filtered_cfg[layer_id])\n",
        "    mask = return_mask(labels)\n",
        "    cfg_mask.append(torch.tensor(mask))\n",
        "    layer_id += 1\n",
        "  elif isinstance(m, nn.MaxPool2d):\n",
        "    layer_id += 1"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 3, 3])\n",
            "torch.Size([64, 32, 3, 3])\n",
            "torch.Size([128, 64, 3, 3])\n",
            "torch.Size([128, 128, 3, 3])\n",
            "torch.Size([256, 128, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n",
            "torch.Size([256, 256, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQIblGXrucTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "540aaa53-ff95-4cfd-d257-b7c9086abd0c"
      },
      "source": [
        "newmodel = vgg(dataset = 'cifar10' ,cfg=filtered_cfg)\n",
        "newmodel.cuda()\n",
        "\n",
        "start_mask = torch.ones(3)\n",
        "layer_id_in_cfg = 0\n",
        "end_mask = cfg_mask[layer_id_in_cfg]\n",
        "for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
        "    if isinstance(m0, nn.BatchNorm2d):\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1,(1,))\n",
        "        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "        m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "        layer_id_in_cfg += 1\n",
        "        start_mask = end_mask\n",
        "        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "            end_mask = cfg_mask[layer_id_in_cfg]\n",
        "    elif isinstance(m0, nn.Conv2d):\n",
        "        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "        if idx0.size == 1:\n",
        "            idx0 = np.resize(idx0, (1,))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1, (1,))\n",
        "        w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
        "        w1 = w1[idx1.tolist(), :, :, :].clone()\n",
        "        m1.weight.data = w1.clone()\n",
        "    elif isinstance(m0, nn.Linear):\n",
        "        if layer_id_in_cfg == len(cfg_mask):\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(cfg_mask[-1].cpu().numpy())))\n",
        "            if idx0.size == 1:\n",
        "                idx0 = np.resize(idx0, (1,))\n",
        "            m1.weight.data = m0.weight.data[:, idx0].clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            continue\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "    elif isinstance(m0, nn.BatchNorm1d):\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "        m1.running_mean = m0.running_mean.clone()\n",
        "        m1.running_var = m0.running_var.clone()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In shape: 3, Out shape 16.\n",
            "In shape: 16, Out shape 32.\n",
            "In shape: 32, Out shape 64.\n",
            "In shape: 64, Out shape 64.\n",
            "In shape: 64, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqTw6x2JVwfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9bd9a3d3-c5f3-4b38-fcb7-de4309d74df2"
      },
      "source": [
        "torch.save({'cfg': filtered_cfg, 'state_dict': newmodel.state_dict()},'Filter_logs/pruned.pth.tar')\n",
        "print(newmodel)\n",
        "model = newmodel\n",
        "model.cuda()\n",
        "acc = test(model)\n",
        "\n",
        "num_parameters = sum([param.nelement() for param in newmodel.parameters()])\n",
        "with open( \"Filter_logs/prune.txt\", \"w\") as fp:\n",
        "    fp.write(\"Number of parameters: \\n\"+str(num_parameters)+\"\\n\")\n",
        "    fp.write(\"Test accuracy: \\n\"+str(acc)+\"\\n\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vgg(\n",
            "  (feature): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (28): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (31): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (35): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (38): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test set: Accuracy: 1000/10000 (10.0%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W60jeRDTgbs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "d18b67c1-e6f6-43d5-9908-344949e7a8a3"
      },
      "source": [
        "ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cmds.xt.txt       \u001b[0m\u001b[01;34mL1_logs\u001b[0m/          \u001b[01;34mmodels\u001b[0m/              vggprune.py\n",
            "compute_flops.py  main_finetune.py  \u001b[01;34m__pycache__\u001b[0m/\n",
            "\u001b[01;34mdata.cifar10\u001b[0m/     \u001b[01;34mMain_logs\u001b[0m/        spherical_kmeans.py\n",
            "\u001b[01;34mFilter_logs\u001b[0m/      main.py           \u001b[01;34mTest_folder\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHonig_-qHq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c06ad89-5337-4f0a-b17a-1d728b427e92"
      },
      "source": [
        "!python vggprune.py --dataset cifar10 --model Main_logs/model_best.pth.tar --save L1_logs/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint 'Main_logs/model_best.pth.tar'\n",
            "=> loaded checkpoint 'Main_logs/model_best.pth.tar' (epoch 151) Prec1: 0.936300\n",
            "Pre-processing Successful!\n",
            "vggprune.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "\n",
            "Test set: Accuracy: 9363/10000 (93.6%)\n",
            "\n",
            "In shape: 3, Out shape 32.\n",
            "In shape: 32, Out shape 64.\n",
            "In shape: 64, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "vgg(\n",
            "  (feature): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (35): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (38): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (41): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test set: Accuracy: 7723/10000 (77.2%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAK6r-xQrV87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f9e6943-ecfa-498e-f44c-7a2d53319970"
      },
      "source": [
        "!python main_finetune.py --refine Filter_logs/pruned.pth.tar --dataset cifar10 --epochs 15 --save Filter_logs/"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 3.237350\n",
            "Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 0.747668\n",
            "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.534155\n",
            "Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 0.653303\n",
            "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.263772\n",
            "Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 0.398468\n",
            "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.427622\n",
            "Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 0.532588\n",
            "main_finetune.py:153: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "\n",
            "Test set: Average loss: 0.4762, Accuracy: 8476/10000 (84.8%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.355136\n",
            "Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 0.268191\n",
            "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.428048\n",
            "Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 0.414867\n",
            "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.338034\n",
            "Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 0.388848\n",
            "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.269209\n",
            "Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 0.242878\n",
            "\n",
            "Test set: Average loss: 0.4133, Accuracy: 8686/10000 (86.9%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.260525\n",
            "Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 0.218600\n",
            "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.346785\n",
            "Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 0.087318\n",
            "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.173258\n",
            "Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 0.333734\n",
            "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.357939\n",
            "Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 0.283821\n",
            "\n",
            "Test set: Average loss: 0.3854, Accuracy: 8812/10000 (88.1%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.092453\n",
            "Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 0.157285\n",
            "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.132612\n",
            "Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 0.195767\n",
            "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.184514\n",
            "Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 0.197534\n",
            "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.171469\n",
            "Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 0.227696\n",
            "\n",
            "Test set: Average loss: 0.3722, Accuracy: 8860/10000 (88.6%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.113764\n",
            "Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 0.160786\n",
            "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.205853\n",
            "Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 0.178125\n",
            "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.232685\n",
            "Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 0.157529\n",
            "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.174645\n",
            "Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 0.133898\n",
            "\n",
            "Test set: Average loss: 0.3633, Accuracy: 8884/10000 (88.8%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.164532\n",
            "Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 0.120697\n",
            "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.171681\n",
            "Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 0.205550\n",
            "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.182198\n",
            "Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.138638\n",
            "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.224415\n",
            "Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.104758\n",
            "\n",
            "Test set: Average loss: 0.3554, Accuracy: 8925/10000 (89.2%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.196148\n",
            "Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.250134\n",
            "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.151838\n",
            "Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.207091\n",
            "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.149676\n",
            "Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 0.152755\n",
            "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.319386\n",
            "Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.229603\n",
            "\n",
            "Test set: Average loss: 0.3621, Accuracy: 8910/10000 (89.1%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.259969\n",
            "Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.260665\n",
            "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.180458\n",
            "Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 0.160205\n",
            "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.108663\n",
            "Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 0.097854\n",
            "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.232254\n",
            "Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.169711\n",
            "\n",
            "Test set: Average loss: 0.3480, Accuracy: 8949/10000 (89.5%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.171766\n",
            "Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.080258\n",
            "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.079865\n",
            "Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.144641\n",
            "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 0.178037\n",
            "Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 0.243420\n",
            "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.106451\n",
            "Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 0.142011\n",
            "\n",
            "Test set: Average loss: 0.3396, Accuracy: 8976/10000 (89.8%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.202996\n",
            "Train Epoch: 9 [6400/50000 (12.8%)]\tLoss: 0.159458\n",
            "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 0.048767\n",
            "Train Epoch: 9 [19200/50000 (38.4%)]\tLoss: 0.163574\n",
            "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 0.155359\n",
            "Train Epoch: 9 [32000/50000 (63.9%)]\tLoss: 0.223093\n",
            "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 0.128769\n",
            "Train Epoch: 9 [44800/50000 (89.5%)]\tLoss: 0.219951\n",
            "\n",
            "Test set: Average loss: 0.3426, Accuracy: 8994/10000 (89.9%)\n",
            "\n",
            "Train Epoch: 10 [0/50000 (0.0%)]\tLoss: 0.127571\n",
            "Train Epoch: 10 [6400/50000 (12.8%)]\tLoss: 0.189455\n",
            "Train Epoch: 10 [12800/50000 (25.6%)]\tLoss: 0.068248\n",
            "Train Epoch: 10 [19200/50000 (38.4%)]\tLoss: 0.035576\n",
            "Train Epoch: 10 [25600/50000 (51.2%)]\tLoss: 0.095501\n",
            "Train Epoch: 10 [32000/50000 (63.9%)]\tLoss: 0.024714\n",
            "Train Epoch: 10 [38400/50000 (76.7%)]\tLoss: 0.053206\n",
            "Train Epoch: 10 [44800/50000 (89.5%)]\tLoss: 0.223621\n",
            "\n",
            "Test set: Average loss: 0.3429, Accuracy: 8999/10000 (90.0%)\n",
            "\n",
            "Train Epoch: 11 [0/50000 (0.0%)]\tLoss: 0.107436\n",
            "Train Epoch: 11 [6400/50000 (12.8%)]\tLoss: 0.159895\n",
            "Train Epoch: 11 [12800/50000 (25.6%)]\tLoss: 0.074515\n",
            "Train Epoch: 11 [19200/50000 (38.4%)]\tLoss: 0.152337\n",
            "Train Epoch: 11 [25600/50000 (51.2%)]\tLoss: 0.059490\n",
            "Train Epoch: 11 [32000/50000 (63.9%)]\tLoss: 0.173187\n",
            "Train Epoch: 11 [38400/50000 (76.7%)]\tLoss: 0.114934\n",
            "Train Epoch: 11 [44800/50000 (89.5%)]\tLoss: 0.033740\n",
            "\n",
            "Test set: Average loss: 0.3445, Accuracy: 9018/10000 (90.2%)\n",
            "\n",
            "Train Epoch: 12 [0/50000 (0.0%)]\tLoss: 0.066697\n",
            "Train Epoch: 12 [6400/50000 (12.8%)]\tLoss: 0.055598\n",
            "Train Epoch: 12 [12800/50000 (25.6%)]\tLoss: 0.050335\n",
            "Train Epoch: 12 [19200/50000 (38.4%)]\tLoss: 0.040887\n",
            "Train Epoch: 12 [25600/50000 (51.2%)]\tLoss: 0.111624\n",
            "Train Epoch: 12 [32000/50000 (63.9%)]\tLoss: 0.070971\n",
            "Train Epoch: 12 [38400/50000 (76.7%)]\tLoss: 0.020678\n",
            "Train Epoch: 12 [44800/50000 (89.5%)]\tLoss: 0.041071\n",
            "\n",
            "Test set: Average loss: 0.3401, Accuracy: 9007/10000 (90.1%)\n",
            "\n",
            "Train Epoch: 13 [0/50000 (0.0%)]\tLoss: 0.116243\n",
            "Train Epoch: 13 [6400/50000 (12.8%)]\tLoss: 0.052247\n",
            "Train Epoch: 13 [12800/50000 (25.6%)]\tLoss: 0.168587\n",
            "Train Epoch: 13 [19200/50000 (38.4%)]\tLoss: 0.150767\n",
            "Train Epoch: 13 [25600/50000 (51.2%)]\tLoss: 0.093461\n",
            "Train Epoch: 13 [32000/50000 (63.9%)]\tLoss: 0.095986\n",
            "Train Epoch: 13 [38400/50000 (76.7%)]\tLoss: 0.177151\n",
            "Train Epoch: 13 [44800/50000 (89.5%)]\tLoss: 0.097998\n",
            "\n",
            "Test set: Average loss: 0.3436, Accuracy: 9030/10000 (90.3%)\n",
            "\n",
            "Train Epoch: 14 [0/50000 (0.0%)]\tLoss: 0.038083\n",
            "Train Epoch: 14 [6400/50000 (12.8%)]\tLoss: 0.055887\n",
            "Train Epoch: 14 [12800/50000 (25.6%)]\tLoss: 0.100455\n",
            "Train Epoch: 14 [19200/50000 (38.4%)]\tLoss: 0.289518\n",
            "Train Epoch: 14 [25600/50000 (51.2%)]\tLoss: 0.038393\n",
            "Train Epoch: 14 [32000/50000 (63.9%)]\tLoss: 0.012877\n",
            "Train Epoch: 14 [38400/50000 (76.7%)]\tLoss: 0.107725\n",
            "Train Epoch: 14 [44800/50000 (89.5%)]\tLoss: 0.259378\n",
            "\n",
            "Test set: Average loss: 0.3446, Accuracy: 9029/10000 (90.3%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1bmjkN1oiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c4004103-f8c0-425b-d24b-e12da8b7a5c4"
      },
      "source": [
        "import compute_flops as flps\n",
        "\n",
        "model = torch.load('L1_logs/model_best.pth.tar')\n",
        "checkpoint = torch.load('L1_logs/checkpoint.pth.tar')\n",
        "model = vgg(dataset='cifar10', depth=16,cfg=cfg)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "flps.print_model_param_nums(model=model)\n",
        "#flps.print_model_param_flops(model=model)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  + Number of params: 5.40M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3k3z1n2A2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
