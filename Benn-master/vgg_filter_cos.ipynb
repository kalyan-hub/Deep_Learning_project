{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg-filter-cos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1e_9HyoNEHF__OHd5f3YUeBh6UnQqJT4I",
      "authorship_tag": "ABX9TyO5JVbpoBttm+9nJwpVUlZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgdshkovi/Benn/blob/master/vgg_filter_cos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUh9Zj8KOgaP",
        "colab_type": "code",
        "outputId": "0f4757c3-bd34-4b07-d196-ece671166a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Z/filter_pruning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Z/filter_pruning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXiovxDcOqo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import math\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from models import *\n",
        "import spherical_kmeans as skm\n",
        "\n",
        "cos = nn.CosineSimilarity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzC-LtjJOql1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batch_size = 256\n",
        "dataset = 'cifar10'\n",
        "cfg = [32, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 256, 256, 256, 'M', 256, 256, 256]\n",
        "filtered_cfg = [16, 32, 'M', 64, 64, 'M', 128, 128, 128, 'M', 128, 128, 128, 'M', 128, 128, 128]\n",
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSThDpoZOqit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('L1_logs/model_best.pth.tar')\n",
        "checkpoint = torch.load('L1_logs/checkpoint.pth.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSOGFI1VOqg4",
        "colab_type": "code",
        "outputId": "4991e1dd-edcc-422d-edd1-5ca1040b2d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = vgg(dataset='cifar10', depth=16,cfg=cfg)\n",
        "model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPTesXsVOqfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model):\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if True else {}\n",
        "    if dataset == 'cifar10':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    elif dataset == 'cifar100':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"No valid dataset is given.\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    return correct / float(len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rFwdhr1Oqdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "def list_duplicates(seq):\n",
        "    tally = defaultdict(list)\n",
        "    for i,item in enumerate(seq):\n",
        "        tally[item].append(i)\n",
        "    return ((key,locs) for key,locs in tally.items() if len(locs)>1)\n",
        "\n",
        "\n",
        "def return_mask(wts, labels):\n",
        "\tmodsource = labels.copy()\n",
        "\t#print(wts)\n",
        "\t#print(labels)\n",
        "\tfor dup in sorted(list_duplicates(labels)):\n",
        "\t    #print(dup)\n",
        "\t    lis = dup[1][1:]\n",
        "\t    a = torch.from_numpy(np.reshape(wts[dup[1][0]],(1,wts[dup[1][0]].size)))\n",
        "\t    #a = wts[dup[1][0]]\n",
        "\t    for i in lis:\n",
        "\t    \tb = torch.from_numpy(np.reshape(wts[i],(1,wts[i].size)))\n",
        "\t    \t#print\n",
        "\t    \tsimi = (cos(a,b))\n",
        "\t    \tif simi>0.5:\n",
        "\t    \t  modsource[i] = -1\n",
        "\n",
        "\tmask = []\n",
        "\tfor el in modsource:\n",
        "\t\tif el!=-1:\n",
        "\t\t\tmask.append(1)\n",
        "\t\telse:\n",
        "\t\t\tmask.append(0)\n",
        "\treturn mask\n",
        "\n",
        "def calc_distance(x1, y1, a, b, c):\n",
        "  d = abs((a * x1 + b * y1 + c)) / (math.sqrt(a * a + b * b))\n",
        "  return d\n",
        "\n",
        "ress = []\n",
        "\n",
        "def optimumk(X, shp):\n",
        "  K = list(range(1,shp//2+1))\n",
        "  K.append(shp)\n",
        "  dist_points_from_cluster_center = []\n",
        "  for no_of_clusters in K:\n",
        "    res = skm.spherical_k_means(X,n_clusters=no_of_clusters,random_state=10)\n",
        "    ress.append(res[1])\n",
        "    dist_points_from_cluster_center.append(res[2])\n",
        "\n",
        "  a = dist_points_from_cluster_center[0] - dist_points_from_cluster_center[shp//2]\n",
        "  b = K[shp//2] - K[0]\n",
        "  c1 = K[0] * dist_points_from_cluster_center[shp//2]\n",
        "  c2 = K[shp//2] * dist_points_from_cluster_center[0]\n",
        "  c = c1 - c2\n",
        "\n",
        "  distance_of_points_from_line = []\n",
        "  for k in range(shp//2+1):\n",
        "    distance_of_points_from_line.append(\n",
        "        calc_distance(K[k], dist_points_from_cluster_center[k], a, b, c))\n",
        "  \n",
        "  optk = distance_of_points_from_line.index(max(distance_of_points_from_line))\n",
        "  return optk\n",
        "\n",
        "\n",
        "def return_cluster_labels(feat_wts, shp):  #no_of_clus):\n",
        "  #res = skm.spherical_k_means(feat_wts,n_clusters=no_of_clus,random_state=10)\n",
        "  #return res[1]\n",
        "\n",
        "  k = optimumk(feat_wts, shp)\n",
        "  print(k)\n",
        "  return ress[k] #= skm.spherical_k_means(feat_wts,n_clusters=k,random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd1dEYfoOqZo",
        "colab_type": "code",
        "outputId": "10c7e546-d9ca-4b45-e647-5532c5d04733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "cos_cfg = []\n",
        "cfg_mask = []\n",
        "layer_id = 0\n",
        "for m in model.modules():\n",
        "  if isinstance(m , nn.Conv2d):\n",
        "    shape = m.weight.data.shape\n",
        "    print(shape)\n",
        "    reshaped_tensor = m.weight.data.clone().numpy().reshape(shape[0] , shape[1]*shape[2]*shape[3])\n",
        "\n",
        "    labels = return_cluster_labels(reshaped_tensor,shape[0])  #filtered_cfg[layer_id])\n",
        "    mask = return_mask(reshaped_tensor,labels)\n",
        "    #print(mask)\n",
        "    print(sum(mask))\n",
        "    cos_cfg.append(sum(mask))\n",
        "    cfg_mask.append(torch.tensor(mask))\n",
        "    layer_id += 1\n",
        "    #break\n",
        "  elif isinstance(m, nn.MaxPool2d):\n",
        "    layer_id += 1\n",
        "    cos_cfg.append('M')\n",
        "print(cos_cfg)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 3, 3])\n",
            "9\n",
            "26\n",
            "torch.Size([64, 32, 3, 3])\n",
            "13\n",
            "32\n",
            "torch.Size([128, 64, 3, 3])\n",
            "22\n",
            "64\n",
            "torch.Size([128, 128, 3, 3])\n",
            "22\n",
            "64\n",
            "torch.Size([256, 128, 3, 3])\n",
            "32\n",
            "64\n",
            "torch.Size([256, 256, 3, 3])\n",
            "52\n",
            "128\n",
            "torch.Size([256, 256, 3, 3])\n",
            "51\n",
            "128\n",
            "torch.Size([256, 256, 3, 3])\n",
            "60\n",
            "128\n",
            "torch.Size([256, 256, 3, 3])\n",
            "57\n",
            "127\n",
            "torch.Size([256, 256, 3, 3])\n",
            "70\n",
            "124\n",
            "torch.Size([256, 256, 3, 3])\n",
            "46\n",
            "62\n",
            "torch.Size([256, 256, 3, 3])\n",
            "40\n",
            "57\n",
            "torch.Size([256, 256, 3, 3])\n",
            "45\n",
            "63\n",
            "[26, 32, 'M', 64, 64, 'M', 64, 128, 128, 'M', 128, 127, 124, 'M', 62, 57, 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDtQEm3LojJj",
        "colab_type": "code",
        "outputId": "a13e4971-434e-49e4-c43a-daca20f72051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(cos_cfg)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[26, 32, 'M', 64, 64, 'M', 64, 128, 128, 'M', 128, 127, 124, 'M', 62, 57, 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD4tz0ECR5aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cos = nn.CosineSimilarity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXNdDVlvRtOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.from_numpy(np.reshape(X[0],(1,X[0].size)))\n",
        "b = torch.from_numpy(np.reshape(X[5],(1,X[5].size)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YLbuGZ3OqWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "3fe77eb0-1c95-4019-82bc-5a6f5f9017d7"
      },
      "source": [
        "newmodel = vgg(dataset = 'cifar10' ,cfg=cos_cfg)\n",
        "newmodel.cuda()\n",
        "\n",
        "start_mask = torch.ones(3)\n",
        "layer_id_in_cfg = 0\n",
        "end_mask = cfg_mask[layer_id_in_cfg]\n",
        "for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
        "    if isinstance(m0, nn.BatchNorm2d):\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1,(1,))\n",
        "        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "        m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "        layer_id_in_cfg += 1\n",
        "        start_mask = end_mask\n",
        "        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "            end_mask = cfg_mask[layer_id_in_cfg]\n",
        "    elif isinstance(m0, nn.Conv2d):\n",
        "        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "        if idx0.size == 1:\n",
        "            idx0 = np.resize(idx0, (1,))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1, (1,))\n",
        "        w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
        "        w1 = w1[idx1.tolist(), :, :, :].clone()\n",
        "        m1.weight.data = w1.clone()\n",
        "    elif isinstance(m0, nn.Linear):\n",
        "        if layer_id_in_cfg == len(cfg_mask):\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(cfg_mask[-1].cpu().numpy())))\n",
        "            if idx0.size == 1:\n",
        "                idx0 = np.resize(idx0, (1,))\n",
        "            m1.weight.data = m0.weight.data[:, idx0].clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            continue\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "    elif isinstance(m0, nn.BatchNorm1d):\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "        m1.running_mean = m0.running_mean.clone()\n",
        "        m1.running_var = m0.running_var.clone()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In shape: 3, Out shape 26.\n",
            "In shape: 26, Out shape 32.\n",
            "In shape: 32, Out shape 64.\n",
            "In shape: 64, Out shape 64.\n",
            "In shape: 64, Out shape 64.\n",
            "In shape: 64, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 127.\n",
            "In shape: 127, Out shape 124.\n",
            "In shape: 124, Out shape 62.\n",
            "In shape: 62, Out shape 57.\n",
            "In shape: 57, Out shape 63.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTqxhffNOqSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "907db380-834e-4ca1-bc8a-03bac86d9468"
      },
      "source": [
        "torch.save({'cfg': cos_cfg, 'state_dict': newmodel.state_dict()},'Test_logs/pruned.pth.tar')\n",
        "print(newmodel)\n",
        "model = newmodel\n",
        "model.cuda()\n",
        "acc = test(model)\n",
        "\n",
        "num_parameters = sum([param.nelement() for param in newmodel.parameters()])\n",
        "with open( \"Test_logs/prune.txt\", \"w\") as fp:\n",
        "    fp.write(\"Number of parameters: \\n\"+str(num_parameters)+\"\\n\")\n",
        "    fp.write(\"Test accuracy: \\n\"+str(acc)+\"\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vgg(\n",
            "  (feature): Sequential(\n",
            "    (0): Conv2d(3, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (28): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(127, 124, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (31): BatchNorm2d(124, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(124, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (35): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(62, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (38): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(57, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (41): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=63, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test set: Accuracy: 1000/10000 (10.0%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDCzLOy_OqPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e49a7de7-ea4a-4744-ca62-94758df22049"
      },
      "source": [
        "!python main_finetune.py --refine Filter_logs/pruned.pth.tar --dataset cifar10 --epochs 10 --save Filter_logs/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 3.237350\n",
            "Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 0.726783\n",
            "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.547180\n",
            "Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 0.646843\n",
            "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.271935\n",
            "Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 0.413913\n",
            "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.418347\n",
            "Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 0.522038\n",
            "main_finetune.py:153: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "\n",
            "Test set: Average loss: 0.4774, Accuracy: 8480/10000 (84.8%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.342160\n",
            "Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 0.275073\n",
            "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.421768\n",
            "Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 0.401331\n",
            "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.335551\n",
            "Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 0.383089\n",
            "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.261973\n",
            "Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 0.254669\n",
            "\n",
            "Test set: Average loss: 0.4138, Accuracy: 8678/10000 (86.8%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.266819\n",
            "Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 0.221399\n",
            "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.337173\n",
            "Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 0.085399\n",
            "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.177809\n",
            "Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 0.333114\n",
            "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.370555\n",
            "Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 0.293188\n",
            "\n",
            "Test set: Average loss: 0.3844, Accuracy: 8810/10000 (88.1%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.096257\n",
            "Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 0.164555\n",
            "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.138722\n",
            "Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 0.204903\n",
            "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.181059\n",
            "Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 0.199728\n",
            "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.149256\n",
            "Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 0.219510\n",
            "\n",
            "Test set: Average loss: 0.3727, Accuracy: 8862/10000 (88.6%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.113797\n",
            "Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 0.160300\n",
            "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.219459\n",
            "Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 0.177086\n",
            "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.220789\n",
            "Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 0.153303\n",
            "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.169538\n",
            "Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 0.130437\n",
            "\n",
            "Test set: Average loss: 0.3636, Accuracy: 8876/10000 (88.8%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.172971\n",
            "Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 0.106796\n",
            "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.168357\n",
            "Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 0.181813\n",
            "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.198001\n",
            "Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.142003\n",
            "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.227023\n",
            "Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.097383\n",
            "\n",
            "Test set: Average loss: 0.3583, Accuracy: 8919/10000 (89.2%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.182942\n",
            "Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.262168\n",
            "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.141101\n",
            "Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.186639\n",
            "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.145180\n",
            "Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 0.164335\n",
            "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.338414\n",
            "Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.255004\n",
            "\n",
            "Test set: Average loss: 0.3615, Accuracy: 8924/10000 (89.2%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.253661\n",
            "Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.217781\n",
            "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.185188\n",
            "Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 0.161850\n",
            "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.115153\n",
            "Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 0.090115\n",
            "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.211271\n",
            "Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.193146\n",
            "\n",
            "Test set: Average loss: 0.3475, Accuracy: 8942/10000 (89.4%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.177031\n",
            "Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.068579\n",
            "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.076584\n",
            "Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.160076\n",
            "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 0.173006\n",
            "Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 0.247607\n",
            "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.092380\n",
            "Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 0.156644\n",
            "\n",
            "Test set: Average loss: 0.3380, Accuracy: 8986/10000 (89.9%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.212222\n",
            "Train Epoch: 9 [6400/50000 (12.8%)]\tLoss: 0.157991\n",
            "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 0.053095\n",
            "Train Epoch: 9 [19200/50000 (38.4%)]\tLoss: 0.164645\n",
            "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 0.145943\n",
            "Train Epoch: 9 [32000/50000 (63.9%)]\tLoss: 0.203535\n",
            "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 0.137545\n",
            "Train Epoch: 9 [44800/50000 (89.5%)]\tLoss: 0.217458\n",
            "\n",
            "Test set: Average loss: 0.3412, Accuracy: 8994/10000 (89.9%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7B_C0maKB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}