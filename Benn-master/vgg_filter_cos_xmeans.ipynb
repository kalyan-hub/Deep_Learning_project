{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg-filter-cos-xmeans.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13yBW6yJ9JKtO3T8Vydrk-cNY2PCRDD57",
      "authorship_tag": "ABX9TyOj2EvkrlUTQEW0P8WouHu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgdshkovi/Benn/blob/master/vgg_filter_cos_xmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUh9Zj8KOgaP",
        "colab_type": "code",
        "outputId": "9a764603-8b85-447f-95f9-7f1f59ebedd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Z/filter_pruning"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Z/filter_pruning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXiovxDcOqo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import math\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from models import *\n",
        "import spherical_kmeans as skm\n",
        "\n",
        "cos = nn.CosineSimilarity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzC-LtjJOql1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batch_size = 256\n",
        "dataset = 'cifar10'\n",
        "cfg = [32, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 256, 256, 256, 'M', 256, 256, 256]\n",
        "filtered_cfg = [16, 32, 'M', 64, 64, 'M', 128, 128, 128, 'M', 128, 128, 128, 'M', 128, 128, 128]\n",
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSThDpoZOqit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('Main_logs/model_best.pth.tar')\n",
        "checkpoint = torch.load('Main_logs/checkpoint.pth.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSOGFI1VOqg4",
        "colab_type": "code",
        "outputId": "29df929b-3933-491f-ef9e-b8fadd8af666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = vgg(dataset='cifar10', depth=16)  #,cfg=cfg)\n",
        "model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPTesXsVOqfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model):\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if True else {}\n",
        "    if dataset == 'cifar10':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10('./data.cifar10', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    elif dataset == 'cifar100':\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR100('./data.cifar100', train=False, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])),\n",
        "            batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"No valid dataset is given.\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    return correct / float(len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rFwdhr1Oqdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from xmeans import XMeans\n",
        "#from xmeans1 import xmeans\n",
        "import spherical_xmeans as sxm\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def list_duplicates(seq):\n",
        "    tally = defaultdict(list)\n",
        "    for i,item in enumerate(seq):\n",
        "        tally[item].append(i)\n",
        "    return ((key,locs) for key,locs in tally.items() if len(locs)>1)\n",
        "\n",
        "\n",
        "def return_mask(wts, labels):\n",
        "\tmodsource = labels.copy()\n",
        "\t#print(wts)\n",
        "\t#print(labels)\n",
        "\tfor dup in sorted(list_duplicates(labels)):\n",
        "\t    #print(dup)\n",
        "\t    lis = dup[1][1:]\n",
        "\t    a = torch.from_numpy(np.reshape(wts[dup[1][0]],(1,wts[dup[1][0]].size)))\n",
        "\t    #a = wts[dup[1][0]]\n",
        "\t    for i in lis:\n",
        "\t    \tb = torch.from_numpy(np.reshape(wts[i],(1,wts[i].size)))\n",
        "\t    \t#print\n",
        "\t    \tsimi = (cos(a,b))\n",
        "\t    \tif simi>0.4:\n",
        "\t    \t  modsource[i] = -1\n",
        "\n",
        "\tmask = []\n",
        "\tfor el in modsource:\n",
        "\t\tif el!=-1:\n",
        "\t\t\tmask.append(1)\n",
        "\t\telse:\n",
        "\t\t\tmask.append(0)\n",
        "\treturn mask\n",
        "\n",
        "def calc_distance(x1, y1, a, b, c):\n",
        "  d = abs((a * x1 + b * y1 + c)) / (math.sqrt(a * a + b * b))\n",
        "  return d\n",
        "\n",
        "ress = []\n",
        "\n",
        "def optimumk(X, shp):\n",
        "  K = list(range(1,shp//2+1))\n",
        "  K.append(shp)\n",
        "  dist_points_from_cluster_center = []\n",
        "  for no_of_clusters in K:\n",
        "    res = skm.spherical_k_means(X,n_clusters=no_of_clusters,random_state=10)\n",
        "    ress.append(res[1])\n",
        "    dist_points_from_cluster_center.append(res[2])\n",
        "\n",
        "  a = dist_points_from_cluster_center[0] - dist_points_from_cluster_center[shp//2]\n",
        "  b = K[shp//2] - K[0]\n",
        "  c1 = K[0] * dist_points_from_cluster_center[shp//2]\n",
        "  c2 = K[shp//2] * dist_points_from_cluster_center[0]\n",
        "  c = c1 - c2\n",
        "\n",
        "  distance_of_points_from_line = []\n",
        "  for k in range(shp//2+1):\n",
        "    distance_of_points_from_line.append(\n",
        "        calc_distance(K[k], dist_points_from_cluster_center[k], a, b, c))\n",
        "  \n",
        "  optk = distance_of_points_from_line.index(max(distance_of_points_from_line))\n",
        "  return optk\n",
        "\n",
        "\n",
        "def return_cluster_labels(feat_wts, shp):  #no_of_clus):\n",
        "  #res = skm.spherical_k_means(feat_wts,n_clusters=no_of_clus,random_state=10)\n",
        "  #return res[1]\n",
        "\n",
        "  #k = optimumk(feat_wts, shp)\n",
        "  #print(k)\n",
        "  #return ress[k] #= skm.spherical_k_means(feat_wts,n_clusters=k,random_state=10)\n",
        "  \n",
        "  #var = XMeans(kmax=shp//2)\n",
        "  #var.fit(feat_wts)\n",
        "  #return var.labels_\n",
        "\n",
        "  centroids, lbls = sxm.XMeansTraining(feat_wts,shp//2)\n",
        "  return lbls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd1dEYfoOqZo",
        "colab_type": "code",
        "outputId": "a5278cb2-2086-4998-8e7b-47e201cbcfe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "cos_cfg = []\n",
        "cfg_mask = []\n",
        "layer_id = 0\n",
        "for m in model.modules():\n",
        "  if isinstance(m , nn.Conv2d):\n",
        "    shape = m.weight.data.shape\n",
        "    print(shape)\n",
        "    reshaped_tensor = m.weight.data.clone().numpy().reshape(shape[0] , shape[1]*shape[2]*shape[3])\n",
        "\n",
        "    labels = return_cluster_labels(reshaped_tensor,shape[0])  #filtered_cfg[layer_id])\n",
        "    mask = return_mask(reshaped_tensor,labels)\n",
        "    #print(mask)\n",
        "    print(len(set(labels)))\n",
        "    print(sum(mask))\n",
        "    cos_cfg.append(sum(mask))\n",
        "    cfg_mask.append(torch.tensor(mask))\n",
        "    layer_id += 1\n",
        "    #break\n",
        "  elif isinstance(m, nn.MaxPool2d):\n",
        "    layer_id += 1\n",
        "    cos_cfg.append('M')\n",
        "print(cos_cfg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 3, 3])\n",
            "10\n",
            "29\n",
            "torch.Size([64, 64, 3, 3])\n",
            "1\n",
            "62\n",
            "torch.Size([128, 64, 3, 3])\n",
            "1\n",
            "128\n",
            "torch.Size([128, 128, 3, 3])\n",
            "1\n",
            "128\n",
            "torch.Size([256, 128, 3, 3])\n",
            "1\n",
            "256\n",
            "torch.Size([256, 256, 3, 3])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9576a2faf85a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mreshaped_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_cluster_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#filtered_cfg[layer_id])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-9dffbe37b0bf>\u001b[0m in \u001b[0;36mreturn_cluster_labels\u001b[0;34m(feat_wts, shp)\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;31m#return var.labels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m   \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXMeansTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_wts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshp\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlbls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Z/filter_pruning/spherical_xmeans.py\u001b[0m in \u001b[0;36mXMeansTraining\u001b[0;34m(data, maxClusters, norm)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mCentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxClusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Z/filter_pruning/spherical_xmeans.py\u001b[0m in \u001b[0;36mXMeans\u001b[0;34m(data, parent, maxClusters)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mCentroids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     '''\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msphericalClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxClusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Z/filter_pruning/spherical_xmeans.py\u001b[0m in \u001b[0;36msphericalClustering\u001b[0;34m(data, parent, maxClusters)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mclModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSphericalKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mclModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mcentroids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Z/filter_pruning/spherical_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Z/filter_pruning/spherical_kmeans.py\u001b[0m in \u001b[0;36mspherical_k_means\u001b[0;34m(X, n_clusters, sample_weight, init, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mbest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mbest_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0mbest_inertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minertia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mbest_n_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_iter_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Z/filter_pruning/spherical_kmeans.py\u001b[0m in \u001b[0;36m_spherical_kmeans_single_lloyd\u001b[0;34m(X, n_clusters, sample_weight, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initialization complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_init_centroids\u001b[0;34m(X, k, init, random_state, x_squared_norms, init_size)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         centers = _k_init(X, k, random_state=random_state,\n\u001b[0;32m--> 626\u001b[0;31m                           x_squared_norms=x_squared_norms)\n\u001b[0m\u001b[1;32m    627\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_k_init\u001b[0;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Compute distances to center candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         distance_to_candidates = euclidean_distances(\n\u001b[0;32m--> 116\u001b[0;31m             X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# update closest distances squared and potential for each candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# To minimize precision issues with float32, we compute the distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# matrix on chunks of X and Y upcast to float64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_euclidean_distances_upcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances_upcast\u001b[0;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mYY_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_chunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDtQEm3LojJj",
        "colab_type": "code",
        "outputId": "b2a3aba5-7cd7-4512-90d5-3e7a9c386ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(cos_cfg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[56, 62, 'M', 128, 128, 'M', 256, 256, 256, 'M', 510, 438, 417, 'M', 173, 160, 169]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD4tz0ECR5aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cos = nn.CosineSimilarity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXNdDVlvRtOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.from_numpy(np.reshape(X[0],(1,X[0].size)))\n",
        "b = torch.from_numpy(np.reshape(X[5],(1,X[5].size)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YLbuGZ3OqWJ",
        "colab_type": "code",
        "outputId": "3ff2de2f-2f3c-4664-ead2-6ebaf411b91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "newmodel = vgg(dataset = 'cifar10' ,cfg=cos_cfg)\n",
        "newmodel.cuda()\n",
        "\n",
        "start_mask = torch.ones(3)\n",
        "layer_id_in_cfg = 0\n",
        "end_mask = cfg_mask[layer_id_in_cfg]\n",
        "for [m0, m1] in zip(model.modules(), newmodel.modules()):\n",
        "    if isinstance(m0, nn.BatchNorm2d):\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1,(1,))\n",
        "        m1.weight.data = m0.weight.data[idx1.tolist()].clone()\n",
        "        m1.bias.data = m0.bias.data[idx1.tolist()].clone()\n",
        "        m1.running_mean = m0.running_mean[idx1.tolist()].clone()\n",
        "        m1.running_var = m0.running_var[idx1.tolist()].clone()\n",
        "        layer_id_in_cfg += 1\n",
        "        start_mask = end_mask\n",
        "        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
        "            end_mask = cfg_mask[layer_id_in_cfg]\n",
        "    elif isinstance(m0, nn.Conv2d):\n",
        "        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
        "        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
        "        print('In shape: {:d}, Out shape {:d}.'.format(idx0.size, idx1.size))\n",
        "        if idx0.size == 1:\n",
        "            idx0 = np.resize(idx0, (1,))\n",
        "        if idx1.size == 1:\n",
        "            idx1 = np.resize(idx1, (1,))\n",
        "        w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
        "        w1 = w1[idx1.tolist(), :, :, :].clone()\n",
        "        m1.weight.data = w1.clone()\n",
        "    elif isinstance(m0, nn.Linear):\n",
        "        if layer_id_in_cfg == len(cfg_mask):\n",
        "            idx0 = np.squeeze(np.argwhere(np.asarray(cfg_mask[-1].cpu().numpy())))\n",
        "            if idx0.size == 1:\n",
        "                idx0 = np.resize(idx0, (1,))\n",
        "            m1.weight.data = m0.weight.data[:, idx0].clone()\n",
        "            m1.bias.data = m0.bias.data.clone()\n",
        "            layer_id_in_cfg += 1\n",
        "            continue\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "    elif isinstance(m0, nn.BatchNorm1d):\n",
        "        m1.weight.data = m0.weight.data.clone()\n",
        "        m1.bias.data = m0.bias.data.clone()\n",
        "        m1.running_mean = m0.running_mean.clone()\n",
        "        m1.running_var = m0.running_var.clone()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In shape: 3, Out shape 56.\n",
            "In shape: 56, Out shape 62.\n",
            "In shape: 62, Out shape 128.\n",
            "In shape: 128, Out shape 128.\n",
            "In shape: 128, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 256.\n",
            "In shape: 256, Out shape 510.\n",
            "In shape: 510, Out shape 438.\n",
            "In shape: 438, Out shape 417.\n",
            "In shape: 417, Out shape 173.\n",
            "In shape: 173, Out shape 160.\n",
            "In shape: 160, Out shape 169.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTqxhffNOqSi",
        "colab_type": "code",
        "outputId": "f8af0f05-abb3-4793-dd6d-4b6fd1d59fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.save({'cfg': cos_cfg, 'state_dict': newmodel.state_dict()},'Xmeans_logs/0.4/pruned.pth.tar')\n",
        "print(newmodel)\n",
        "model = newmodel\n",
        "model.cuda()\n",
        "acc = test(model)\n",
        "\n",
        "num_parameters = sum([param.nelement() for param in newmodel.parameters()])\n",
        "with open( \"Xmeans_logs/0.4/prune.txt\", \"w\") as fp:\n",
        "    fp.write(\"Number of parameters: \\n\"+str(num_parameters)+\"\\n\")\n",
        "    fp.write(\"Test accuracy: \\n\"+str(acc)+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vgg(\n",
            "  (feature): Sequential(\n",
            "    (0): Conv2d(3, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(62, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (25): BatchNorm2d(510, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(510, 438, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (28): BatchNorm2d(438, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(438, 417, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (31): BatchNorm2d(417, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(417, 173, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (35): BatchNorm2d(173, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(173, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (38): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(160, 169, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (41): BatchNorm2d(169, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=169, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Test set: Accuracy: 1629/10000 (16.3%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDCzLOy_OqPK",
        "colab_type": "code",
        "outputId": "e49a7de7-ea4a-4744-ca62-94758df22049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python main_finetune.py --refine Filter_logs/pruned.pth.tar --dataset cifar10 --epochs 10 --save Filter_logs/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Train Epoch: 0 [0/50000 (0.0%)]\tLoss: 3.237350\n",
            "Train Epoch: 0 [6400/50000 (12.8%)]\tLoss: 0.726783\n",
            "Train Epoch: 0 [12800/50000 (25.6%)]\tLoss: 0.547180\n",
            "Train Epoch: 0 [19200/50000 (38.4%)]\tLoss: 0.646843\n",
            "Train Epoch: 0 [25600/50000 (51.2%)]\tLoss: 0.271935\n",
            "Train Epoch: 0 [32000/50000 (63.9%)]\tLoss: 0.413913\n",
            "Train Epoch: 0 [38400/50000 (76.7%)]\tLoss: 0.418347\n",
            "Train Epoch: 0 [44800/50000 (89.5%)]\tLoss: 0.522038\n",
            "main_finetune.py:153: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "\n",
            "Test set: Average loss: 0.4774, Accuracy: 8480/10000 (84.8%)\n",
            "\n",
            "Train Epoch: 1 [0/50000 (0.0%)]\tLoss: 0.342160\n",
            "Train Epoch: 1 [6400/50000 (12.8%)]\tLoss: 0.275073\n",
            "Train Epoch: 1 [12800/50000 (25.6%)]\tLoss: 0.421768\n",
            "Train Epoch: 1 [19200/50000 (38.4%)]\tLoss: 0.401331\n",
            "Train Epoch: 1 [25600/50000 (51.2%)]\tLoss: 0.335551\n",
            "Train Epoch: 1 [32000/50000 (63.9%)]\tLoss: 0.383089\n",
            "Train Epoch: 1 [38400/50000 (76.7%)]\tLoss: 0.261973\n",
            "Train Epoch: 1 [44800/50000 (89.5%)]\tLoss: 0.254669\n",
            "\n",
            "Test set: Average loss: 0.4138, Accuracy: 8678/10000 (86.8%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0.0%)]\tLoss: 0.266819\n",
            "Train Epoch: 2 [6400/50000 (12.8%)]\tLoss: 0.221399\n",
            "Train Epoch: 2 [12800/50000 (25.6%)]\tLoss: 0.337173\n",
            "Train Epoch: 2 [19200/50000 (38.4%)]\tLoss: 0.085399\n",
            "Train Epoch: 2 [25600/50000 (51.2%)]\tLoss: 0.177809\n",
            "Train Epoch: 2 [32000/50000 (63.9%)]\tLoss: 0.333114\n",
            "Train Epoch: 2 [38400/50000 (76.7%)]\tLoss: 0.370555\n",
            "Train Epoch: 2 [44800/50000 (89.5%)]\tLoss: 0.293188\n",
            "\n",
            "Test set: Average loss: 0.3844, Accuracy: 8810/10000 (88.1%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0.0%)]\tLoss: 0.096257\n",
            "Train Epoch: 3 [6400/50000 (12.8%)]\tLoss: 0.164555\n",
            "Train Epoch: 3 [12800/50000 (25.6%)]\tLoss: 0.138722\n",
            "Train Epoch: 3 [19200/50000 (38.4%)]\tLoss: 0.204903\n",
            "Train Epoch: 3 [25600/50000 (51.2%)]\tLoss: 0.181059\n",
            "Train Epoch: 3 [32000/50000 (63.9%)]\tLoss: 0.199728\n",
            "Train Epoch: 3 [38400/50000 (76.7%)]\tLoss: 0.149256\n",
            "Train Epoch: 3 [44800/50000 (89.5%)]\tLoss: 0.219510\n",
            "\n",
            "Test set: Average loss: 0.3727, Accuracy: 8862/10000 (88.6%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0.0%)]\tLoss: 0.113797\n",
            "Train Epoch: 4 [6400/50000 (12.8%)]\tLoss: 0.160300\n",
            "Train Epoch: 4 [12800/50000 (25.6%)]\tLoss: 0.219459\n",
            "Train Epoch: 4 [19200/50000 (38.4%)]\tLoss: 0.177086\n",
            "Train Epoch: 4 [25600/50000 (51.2%)]\tLoss: 0.220789\n",
            "Train Epoch: 4 [32000/50000 (63.9%)]\tLoss: 0.153303\n",
            "Train Epoch: 4 [38400/50000 (76.7%)]\tLoss: 0.169538\n",
            "Train Epoch: 4 [44800/50000 (89.5%)]\tLoss: 0.130437\n",
            "\n",
            "Test set: Average loss: 0.3636, Accuracy: 8876/10000 (88.8%)\n",
            "\n",
            "Train Epoch: 5 [0/50000 (0.0%)]\tLoss: 0.172971\n",
            "Train Epoch: 5 [6400/50000 (12.8%)]\tLoss: 0.106796\n",
            "Train Epoch: 5 [12800/50000 (25.6%)]\tLoss: 0.168357\n",
            "Train Epoch: 5 [19200/50000 (38.4%)]\tLoss: 0.181813\n",
            "Train Epoch: 5 [25600/50000 (51.2%)]\tLoss: 0.198001\n",
            "Train Epoch: 5 [32000/50000 (63.9%)]\tLoss: 0.142003\n",
            "Train Epoch: 5 [38400/50000 (76.7%)]\tLoss: 0.227023\n",
            "Train Epoch: 5 [44800/50000 (89.5%)]\tLoss: 0.097383\n",
            "\n",
            "Test set: Average loss: 0.3583, Accuracy: 8919/10000 (89.2%)\n",
            "\n",
            "Train Epoch: 6 [0/50000 (0.0%)]\tLoss: 0.182942\n",
            "Train Epoch: 6 [6400/50000 (12.8%)]\tLoss: 0.262168\n",
            "Train Epoch: 6 [12800/50000 (25.6%)]\tLoss: 0.141101\n",
            "Train Epoch: 6 [19200/50000 (38.4%)]\tLoss: 0.186639\n",
            "Train Epoch: 6 [25600/50000 (51.2%)]\tLoss: 0.145180\n",
            "Train Epoch: 6 [32000/50000 (63.9%)]\tLoss: 0.164335\n",
            "Train Epoch: 6 [38400/50000 (76.7%)]\tLoss: 0.338414\n",
            "Train Epoch: 6 [44800/50000 (89.5%)]\tLoss: 0.255004\n",
            "\n",
            "Test set: Average loss: 0.3615, Accuracy: 8924/10000 (89.2%)\n",
            "\n",
            "Train Epoch: 7 [0/50000 (0.0%)]\tLoss: 0.253661\n",
            "Train Epoch: 7 [6400/50000 (12.8%)]\tLoss: 0.217781\n",
            "Train Epoch: 7 [12800/50000 (25.6%)]\tLoss: 0.185188\n",
            "Train Epoch: 7 [19200/50000 (38.4%)]\tLoss: 0.161850\n",
            "Train Epoch: 7 [25600/50000 (51.2%)]\tLoss: 0.115153\n",
            "Train Epoch: 7 [32000/50000 (63.9%)]\tLoss: 0.090115\n",
            "Train Epoch: 7 [38400/50000 (76.7%)]\tLoss: 0.211271\n",
            "Train Epoch: 7 [44800/50000 (89.5%)]\tLoss: 0.193146\n",
            "\n",
            "Test set: Average loss: 0.3475, Accuracy: 8942/10000 (89.4%)\n",
            "\n",
            "Train Epoch: 8 [0/50000 (0.0%)]\tLoss: 0.177031\n",
            "Train Epoch: 8 [6400/50000 (12.8%)]\tLoss: 0.068579\n",
            "Train Epoch: 8 [12800/50000 (25.6%)]\tLoss: 0.076584\n",
            "Train Epoch: 8 [19200/50000 (38.4%)]\tLoss: 0.160076\n",
            "Train Epoch: 8 [25600/50000 (51.2%)]\tLoss: 0.173006\n",
            "Train Epoch: 8 [32000/50000 (63.9%)]\tLoss: 0.247607\n",
            "Train Epoch: 8 [38400/50000 (76.7%)]\tLoss: 0.092380\n",
            "Train Epoch: 8 [44800/50000 (89.5%)]\tLoss: 0.156644\n",
            "\n",
            "Test set: Average loss: 0.3380, Accuracy: 8986/10000 (89.9%)\n",
            "\n",
            "Train Epoch: 9 [0/50000 (0.0%)]\tLoss: 0.212222\n",
            "Train Epoch: 9 [6400/50000 (12.8%)]\tLoss: 0.157991\n",
            "Train Epoch: 9 [12800/50000 (25.6%)]\tLoss: 0.053095\n",
            "Train Epoch: 9 [19200/50000 (38.4%)]\tLoss: 0.164645\n",
            "Train Epoch: 9 [25600/50000 (51.2%)]\tLoss: 0.145943\n",
            "Train Epoch: 9 [32000/50000 (63.9%)]\tLoss: 0.203535\n",
            "Train Epoch: 9 [38400/50000 (76.7%)]\tLoss: 0.137545\n",
            "Train Epoch: 9 [44800/50000 (89.5%)]\tLoss: 0.217458\n",
            "\n",
            "Test set: Average loss: 0.3412, Accuracy: 8994/10000 (89.9%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7B_C0maKB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}