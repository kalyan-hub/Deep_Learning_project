{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled3.ipynb","provenance":[],"authorship_tag":"ABX9TyMiC3xVxwCw1XUumFNiyRap"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Cx9RoM9SKAAW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d2712faa-c381-4fd1-b223-316ee05aa8a5","executionInfo":{"status":"ok","timestamp":1589963337731,"user_tz":-330,"elapsed":9656,"user":{"displayName":"kalyan nsv","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_fmntQ9PKm5rVjsiZ0ZAwUbtysrsW_YNe3ruEeA=s64","userId":"17365169807449933549"}}},"source":["import math\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","\n","__all__ = ['vgg']\n","\n","defaultcfg = {\n","    11 : [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n","    13 : [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n","    16 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n","    19 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n","}\n","\n","class vgg(nn.Module):\n","    def __init__(self, dataset='cifar10', depth=19, init_weights=True, cfg=None):\n","        super(vgg, self).__init__()\n","        if cfg is None:\n","            cfg = defaultcfg[depth]\n","\n","        self.cfg = cfg\n","\n","        self.feature = self.make_layers(cfg, True)\n","\n","        if dataset == 'cifar10':\n","            num_classes = 10\n","        elif dataset == 'cifar100':\n","            num_classes = 100\n","        self.classifier = nn.Sequential(\n","              nn.Linear(cfg[-1], 512),\n","              nn.BatchNorm1d(512),\n","              nn.ReLU(inplace=True),\n","              nn.Linear(512, num_classes)\n","            )\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def make_layers(self, cfg, batch_norm=False):\n","        layers = []\n","        in_channels = 3\n","        for v in cfg:\n","            if v == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n","                if batch_norm:\n","                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","                else:\n","                    layers += [conv2d, nn.ReLU(inplace=True)]\n","                in_channels = v\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.feature(x)\n","        x = nn.AvgPool2d(2)(x)\n","        x = x.view(x.size(0), -1)\n","        y = self.classifier(x)\n","        return y\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(0.5)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()\n","\n","if __name__ == '__main__':\n","    net = vgg()\n","    x = Variable(torch.FloatTensor(16, 3, 40, 40))\n","    y = net(x)\n","    print(y.data.shape)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["torch.Size([16, 10])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KcjR4oOxKOcL","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjPZmu-VMbAY","colab_type":"code","colab":{}},"source":["# Code from https://github.com/simochen/model-tools.\n","import numpy as np\n","\n","import torch\n","import torchvision\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","\n","def print_model_param_nums(model=None, multiply_adds=True):\n","    if model == None:\n","        model = torchvision.models.alexnet()\n","    total = sum([param.nelement() for param in model.parameters()])\n","    print('  + Number of params: %.2fM' % (total / 1e6))\n","\n","def print_model_param_flops(model=None, input_res=224, multiply_adds=True):\n","\n","    prods = {}\n","    def save_hook(name):\n","        def hook_per(self, input, output):\n","            prods[name] = np.prod(input[0].shape)\n","        return hook_per\n","\n","    list_1=[]\n","    def simple_hook(self, input, output):\n","        list_1.append(np.prod(input[0].shape))\n","    list_2={}\n","    def simple_hook2(self, input, output):\n","        list_2['names'] = np.prod(input[0].shape)\n","\n","    list_conv=[]\n","    def conv_hook(self, input, output):\n","        batch_size, input_channels, input_height, input_width = input[0].size()\n","        output_channels, output_height, output_width = output[0].size()\n","\n","        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n","        bias_ops = 1 if self.bias is not None else 0\n","\n","        params = output_channels * (kernel_ops + bias_ops)\n","        flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n","\n","        list_conv.append(flops)\n","\n","    list_linear=[]\n","    def linear_hook(self, input, output):\n","        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n","\n","        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n","        bias_ops = self.bias.nelement()\n","\n","        flops = batch_size * (weight_ops + bias_ops)\n","        list_linear.append(flops)\n","\n","    list_bn=[]\n","    def bn_hook(self, input, output):\n","        list_bn.append(input[0].nelement() * 2)\n","\n","    list_relu=[]\n","    def relu_hook(self, input, output):\n","        list_relu.append(input[0].nelement())\n","\n","    list_pooling=[]\n","    def pooling_hook(self, input, output):\n","        batch_size, input_channels, input_height, input_width = input[0].size()\n","        output_channels, output_height, output_width = output[0].size()\n","\n","        kernel_ops = self.kernel_size * self.kernel_size\n","        bias_ops = 0\n","        params = 0\n","        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n","\n","        list_pooling.append(flops)\n","\n","    list_upsample=[]\n","    # For bilinear upsample\n","    def upsample_hook(self, input, output):\n","        batch_size, input_channels, input_height, input_width = input[0].size()\n","        output_channels, output_height, output_width = output[0].size()\n","\n","        flops = output_height * output_width * output_channels * batch_size * 12\n","        list_upsample.append(flops)\n","\n","    def foo(net):\n","        childrens = list(net.children())\n","        if not childrens:\n","            if isinstance(net, torch.nn.Conv2d):\n","                net.register_forward_hook(conv_hook)\n","            if isinstance(net, torch.nn.Linear):\n","                net.register_forward_hook(linear_hook)\n","            if isinstance(net, torch.nn.BatchNorm2d):\n","                net.register_forward_hook(bn_hook)\n","            if isinstance(net, torch.nn.ReLU):\n","                net.register_forward_hook(relu_hook)\n","            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n","                net.register_forward_hook(pooling_hook)\n","            if isinstance(net, torch.nn.Upsample):\n","                net.register_forward_hook(upsample_hook)\n","            return\n","        for c in childrens:\n","            foo(c)\n","\n","    if model == None:\n","        model = torchvision.models.alexnet()\n","    foo(model)\n","    input = Variable(torch.rand(3, 3, input_res, input_res), requires_grad = True)\n","    out = model(input)\n","\n","\n","    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n","\n","    print('  + Number of FLOPs: %.5fG' % (total_flops / 3 / 1e9))\n","    \n","    return total_flops / 3\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9edOjTxNDhJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":316},"outputId":"cb30e82b-1caa-42a0-a242-9246a56b760a","executionInfo":{"status":"error","timestamp":1589963438991,"user_tz":-330,"elapsed":3203,"user":{"displayName":"kalyan nsv","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_fmntQ9PKm5rVjsiZ0ZAwUbtysrsW_YNe3ruEeA=s64","userId":"17365169807449933549"}}},"source":["from .vgg import *\n","from .resnet import *"],"execution_count":31,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-5f115153ec20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.vgg'; '__main__' is not a package","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}